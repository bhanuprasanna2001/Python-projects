# ETL Pipeline Configuration
# This file defines data sources, transformation rules, and loading targets

pipeline:
  name: "multi-source-etl"
  description: "Aggregates data from GitHub, CSV files, and SQLite databases"
  
# Data Sources Configuration
sources:
  github:
    type: github_api
    enabled: true
    # Fetch starred repositories and their metadata
    endpoint: "users/{username}/starred"
    username: "torvalds"  # Default user to fetch data for
    max_items: 100
    rate_limit_delay: 1.0  # seconds between requests
    
  weather:
    type: csv
    enabled: true
    # Public weather dataset (you can replace with any CSV URL or path)
    path: "data/raw/weather_sample.csv"
    # For demonstration, we'll generate sample data if not exists
    
  books:
    type: sqlite
    enabled: true
    # Extract from web scraper database (Project 1.2 output)
    database_path: "../1.2 - Web Scraper/data/books.db"
    query: "SELECT * FROM books"
    fallback_path: "data/raw/books_sample.db"  # If web scraper DB not available

# Transformation Configuration
transformations:
  # Standardize all data to common schema
  normalize_dates: true
  handle_missing: "fill_default"  # Options: drop, fill_default, fill_mean
  deduplicate: true
  
  # Data quality thresholds
  quality:
    min_completeness: 0.8  # 80% of required fields must be present
    max_duplicates_ratio: 0.05  # Allow up to 5% duplicates before warning

# Loading Configuration
loading:
  target: sqlite  # Options: sqlite, postgres
  
  sqlite:
    path: "data/output/etl_output.db"
    
  postgres:
    host: ${POSTGRES_HOST:-localhost}
    port: ${POSTGRES_PORT:-5432}
    database: ${POSTGRES_DB:-etl_pipeline}
    user: ${POSTGRES_USER:-etl_user}
    password: ${POSTGRES_PASSWORD:-etl_password}
  
  # Upsert behavior
  on_conflict: "update"  # Options: skip, update, fail
  batch_size: 1000

# Scheduling Configuration
scheduling:
  enabled: false  # Enable for production
  timezone: "Europe/Berlin"
  jobs:
    - name: "hourly_sync"
      cron: "0 * * * *"  # Every hour
      stages: ["extract", "transform", "load"]
      
    - name: "daily_full_refresh"
      cron: "0 2 * * *"  # 2 AM daily
      stages: ["extract", "transform", "load"]
      full_refresh: true

# Monitoring Configuration
monitoring:
  metrics_enabled: true
  metrics_path: "data/metrics/pipeline_metrics.json"
  
  alerts:
    enabled: false  # Enable when webhook is configured
    webhook_url: ${ALERT_WEBHOOK_URL:-}
    on_failure: true
    on_degraded_quality: true
    min_records_threshold: 10  # Alert if fewer records than expected

# Logging Configuration
logging:
  level: INFO
  format: "structured"  # Options: structured (JSON), simple
  file_path: "logs/pipeline.log"
  max_size_mb: 10
  backup_count: 5
